{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ced7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, logging\n",
    "import time\n",
    "\n",
    "logging.set_verbosity_error() # this just clears our cell output of some clutter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196ec08",
   "metadata": {},
   "source": [
    "We'll first use the `torch.cuda` module to learn more about the GPUs we have available.\n",
    "\n",
    "- you can find out whether or not a GPU is available with `torch.cuda.is_available()`\n",
    "- you can find out *how many* GPUs are available with `torch.cuda.device_count()`\n",
    "- you can find out what the name of the GPUs are, given they exist, with `torch.cuda.get_device_name()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3220dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda availabel: False\n",
      "Count of device: 0\n",
      "Name of the device: No device is available.\n"
     ]
    }
   ],
   "source": [
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "count_of_device = torch.cuda.device_count()\n",
    "\n",
    "if count_of_device:\n",
    "    name_of_device = torch.cuda.get_device_name()\n",
    "else:\n",
    "    name_of_device = \"No device is available.\"\n",
    "\n",
    "print(f\"Is cuda availabel: {is_cuda_available}\")\n",
    "print(f\"Count of device: {count_of_device}\")\n",
    "print(f\"Name of the device: {name_of_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89f3d1",
   "metadata": {},
   "source": [
    "One very common pattern you'll see in machine learning code is encapsulated in the following line:\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "It finds out whether or not we have a GPU available and, if so, assigns it to `device`. Otherwise, `device` is `\"cpu\"`. This pattern enables what's called \"device-agnostic code,\" meaning the code itself doesn't know whether or not we've got GPU access, but will work in either case.\n",
    "\n",
    "Datasets and models are moved to our `device` using the `.to()` method. For instance, to initialize a 3x3 PyTorch tensor with random weights and then assign it to the GPU, we'd write\n",
    "\n",
    "```python\n",
    "tensor = torch.rand(3,3).to(device)\n",
    "```\n",
    "Moving a model to the GPU is the same method. For model `ML_model`, you'd simply write `ML_model.to(device)` (assuming we've defined `device` the way we did above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b81548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: cpu\n",
      "Following tensor added: tensor([[0.1280, 0.2532],\n",
      "        [0.7394, 0.1501]])\n",
      "Model added to: cpu\n"
     ]
    }
   ],
   "source": [
    "# device should be 'cuda' if GPU is available, else cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Lets create the random tensor matrix and put it on device.\n",
    "tensor = torch.rand(2,2).to(device)\n",
    "\n",
    "# Lets import the tokenizer model and put it to particular device \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Device name: {device}\")\n",
    "print(f\"Following tensor added: {tensor}\")\n",
    "print(f\"Model added to: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507dded",
   "metadata": {},
   "source": [
    "Notice how the model's device is returned as `\"cuda:0\"`? That's because it's indexed as the *first* GPU in what might be an array of many GPUs. We only have one GPU we'll use, but in larger scale training runs, you might find additional GPUs assigned to `\"cuda:1\"`, `\"cuda:2\"`, and so on.\n",
    "\n",
    "Finally, let's perform a small experiment to see how much the GPU speeds up our computation.\n",
    "\n",
    "First, assign to the variable `cpu_tensor` a randomly-initialized PyTorch tensor with 10,000 rows and 10,000 columns. You can do so with `torch.rand(num_rows, num_columns)`. We'll then add all the values together using the `.sum()` method.\n",
    "\n",
    "Then, under the `torch.cuda.is_available()` if check, assign to the variable `gpu_tensor` another randomized 10,000 x 10,000 tensor, but pass `device=device` as the third argument.\n",
    "\n",
    "Execute the code cell to see how much faster the GPU can sum every number in a 10,000 by 10,000 tensor than the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f26f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time:  0.08378267288208008\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "## YOUR SOLUTION HERE ##\n",
    "cpu_tensor = torch.rand(1000, 1000)\n",
    "cpu_sum = cpu_tensor.sum()\n",
    "cpu_time = time.time() - start_time\n",
    "print(\"CPU time: \", cpu_time)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    start_time = time.time()\n",
    "## YOUR SOLUTION HERE ##\n",
    "    gpu_tensor = torch.rand(1000, 1000, device=device)\n",
    "    gpu_sum = gpu_tensor.sum()\n",
    "    gpu_time = time.time() - start_time\n",
    "    print(\"GPU time: \", gpu_time)\n",
    "    print(\"Speedup: \", cpu_time / gpu_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
